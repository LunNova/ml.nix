diff --git a/test/inductor/test_flex_attention.py b/test/inductor/test_flex_attention.py
index 282828cda42ead..8da44d8cdc547e 100644
--- a/test/inductor/test_flex_attention.py
+++ b/test/inductor/test_flex_attention.py
@@ -3286,6 +3286,11 @@ def apply_multiplicative_bias(score, b, h, q_idx, kv_idx):
 
         self.run_test_with_call(attention, Q_S=Q_S, KV_S=KV_S)
 
+    @supported_platform
+    def test_num_warps_8_error(self):
+        attention = functools.partial(flex_attention, score_mod=_identity)
+        self.run_test_with_call(attention, Q_S=128, KV_S=128, Q_D=128, V_D=128)
+
     @unittest.skipIf(not TEST_MULTIGPU, "detected only one GPU")
     def test_qkv_and_block_mask_on_the_same_device(self):
         make_tensor = functools.partial(
@@ -4040,6 +4045,36 @@ def doc_mask_mod(b, h, q_idx, kv_idx):
             block_mask = create_block_mask(doc_mask_mod, None, None, 1024 + i, 1024 + i)
             torch.compile(flex_attention)(q, k, v, block_mask=block_mask)
 
+    @supported_platform
+    def test_eager_tracing_correctness(self):
+        qk_dims = 64
+        v_dims = 128
+        q_heads = 4
+        kv_heads = 2
+        seq_len = 256
+        batch_size = 1
+
+        make_tensor = functools.partial(torch.randn, device="cuda", dtype=torch.float16)
+        q = make_tensor(*(batch_size, q_heads, seq_len, qk_dims))
+        k = make_tensor(*(batch_size, kv_heads, seq_len, qk_dims))
+        v = make_tensor(*(batch_size, kv_heads, seq_len, v_dims))
+
+        def flex_attention_fn():
+            out = flex_attention(q, k, v, enable_gqa=True)
+            return out.view(batch_size, q_heads, seq_len, 2, 64)
+
+        # Run with compilation
+        compiled_fn = torch.compile(flex_attention_fn, fullgraph=True)
+        result = compiled_fn()
+
+        # Assert expected output shape
+        expected_shape = (batch_size, q_heads, seq_len, 2, 64)
+        self.assertEqual(
+            result.shape,
+            expected_shape,
+            f"Expected output shape {expected_shape}, but got {result.shape}",
+        )
+
     @common_utils.parametrize("compile", [False, True])
     @supported_platform
     def test_block_mask_vs_sequence_lengths(self, compile):
diff --git a/test/inductor/test_metrics.py b/test/inductor/test_metrics.py
index 90d6b0132e1761..e0c6a091993ca2 100644
--- a/test/inductor/test_metrics.py
+++ b/test/inductor/test_metrics.py
@@ -3,7 +3,7 @@
 from torch._inductor import config, metrics
 from torch._inductor.test_case import run_tests, TestCase
 from torch._inductor.utils import collect_defined_kernels
-from torch._inductor.wrapper_benchmark import get_kernel_category_by_source_code
+from torch._inductor.wrapper_benchmark import KernelCategory
 from torch.testing._internal.common_device_type import largeTensorTest
 from torch.testing._internal.inductor_utils import GPU_TYPE, HAS_GPU
 
@@ -71,10 +71,10 @@ def test_count_pattern(self):
         self.assertEqual(1, metrics._count_pattern(proper_kernel_fn_code, "for "))
 
     def test_parse_reduction_hint(self):
-        kernel_category = get_kernel_category_by_source_code(example_kernel)
-        self.assertEqual("reduction", kernel_category)
+        kernel_category = KernelCategory.from_source_code(example_kernel)
+        self.assertEqual(KernelCategory.REDUCTION, kernel_category)
         self.assertEqual(
-            "INNER", metrics._parse_reduction_hint(kernel_category, example_kernel)
+            "INNER", metrics._parse_reduction_hint(example_kernel, kernel_category)
         )
 
     @config.patch("fx_graph_remote_cache", False)
@@ -111,7 +111,8 @@ def f(x):
         self.assertEqual(len(kernel_list), 1)
         kernel_code = kernel_list[0]
         self.assertEqual(
-            metrics._parse_kernel_args_num_gb(kernel_code, "pointwise"), 2.0
+            metrics._parse_kernel_args_num_gb(kernel_code),
+            2.0,
         )
 
 
diff --git a/torch/_dynamo/variables/higher_order_ops.py b/torch/_dynamo/variables/higher_order_ops.py
index 679ddee54bbb45..1df7fff6a9f475 100644
--- a/torch/_dynamo/variables/higher_order_ops.py
+++ b/torch/_dynamo/variables/higher_order_ops.py
@@ -2272,6 +2272,8 @@ def call_function(
         args: "List[VariableTracker]",
         kwargs: "Dict[str, VariableTracker]",
     ) -> "VariableTracker":
+        from torch._higher_order_ops.flex_attention import flex_attention_fake_impl
+
         from .builder import wrap_fx_proxy
 
         (
@@ -2309,12 +2311,9 @@ def call_function(
         inp_args, _ = proxy_args_kwargs(proxied_args, {})
 
         query_meta = query.as_proxy().node.meta["example_value"]
+        value_meta = value.as_proxy().node.meta["example_value"]
         with torch._guards.TracingContext.try_get().fake_mode:
-            out_meta = torch.empty_like(
-                query_meta, memory_format=torch.contiguous_format
-            )
-            # TODO: Figure out a better way to handle this for NJT than using sum()
-            lse_meta = torch.empty_like(query_meta, dtype=torch.float32).sum(dim=-1)
+            out_meta, lse_meta = flex_attention_fake_impl(query_meta, value_meta)
         example_value = (out_meta, lse_meta)
 
         # Compose the ordered HOO args:
diff --git a/torch/_higher_order_ops/flex_attention.py b/torch/_higher_order_ops/flex_attention.py
index 7448a5eb5984c7..d1f7ddc93d4d04 100644
--- a/torch/_higher_order_ops/flex_attention.py
+++ b/torch/_higher_order_ops/flex_attention.py
@@ -441,6 +441,24 @@ def flex_attention_functionalize(
     return ctx.wrap_tensors(out)  # type: ignore[return-value, arg-type]
 
 
+def flex_attention_fake_impl(
+    query: torch.Tensor, value: torch.Tensor
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    # TODO: Figure out a better way to handle this for NJT than using sum()
+    if query.is_nested:
+        out = torch.empty_like(query, memory_format=torch.contiguous_format)
+        logsumexp = query.sum(dim=-1)
+        return out, logsumexp
+
+    v_head_dim = value.size(-1)
+    batch_size, num_heads, seq_len_q, _q_head_dim = query.shape
+    logsumexp = query.new_empty(batch_size, num_heads, seq_len_q, dtype=torch.float32)
+    out_shape = (batch_size, num_heads, seq_len_q, v_head_dim)
+    out = query.new_empty(out_shape)
+    out = _permute_strides(out, query.stride())
+    return out, logsumexp
+
+
 @flex_attention.py_impl(FakeTensorMode)
 def flex_attention_fake_tensor_mode(
     mode: FakeTensorMode,
@@ -455,14 +473,7 @@ def flex_attention_fake_tensor_mode(
     mask_mod_other_buffers: Tuple = (),
 ) -> Tuple[torch.Tensor, torch.Tensor]:
     with mode:
-        v_head_dim = value.size(-1)
-        batch_size, num_heads, seq_len_q, _q_head_dim = query.shape
-        logsumexp = query.new_empty(
-            batch_size, num_heads, seq_len_q, dtype=torch.float32
-        )
-        out_shape = (batch_size, num_heads, seq_len_q, v_head_dim)
-        out = query.new_empty(out_shape)
-        out = _permute_strides(out, query.stride())
+        out, logsumexp = flex_attention_fake_impl(query, value)
         return out, logsumexp
 
 
diff --git a/torch/_inductor/codegen/triton.py b/torch/_inductor/codegen/triton.py
index 73492b7edf9ecf..9aa50e1c9399c1 100644
--- a/torch/_inductor/codegen/triton.py
+++ b/torch/_inductor/codegen/triton.py
@@ -69,7 +69,7 @@
     upcast_compute_type,
 )
 from ..virtualized import _ops as ops, OpsHandler, ReductionType, StoreMode, V
-from ..wrapper_benchmark import get_kernel_category_by_source_code
+from ..wrapper_benchmark import KernelCategory
 from .block_analysis import BlockPatternMatcher
 from .common import (
     BackendFeature,
@@ -3715,9 +3715,19 @@ def define_kernel(self, src_code, node_schedule, kernel):
                 if config.triton.descriptive_names
                 else ""
             )
-            kernel_category = get_kernel_category_by_source_code(src_code)[:3]
+            kernel_category = KernelCategory.from_source_code(src_code)
+            fused_name = (
+                kernel.kernel_name
+                if kernel_category == KernelCategory.TEMPLATE
+                else fused_name
+            )
             kernel_name = "_".join(
-                ["triton", kernel_category, fused_name, wrapper.next_kernel_suffix()]
+                [
+                    "triton",
+                    kernel_category.abbrev,
+                    fused_name,
+                    wrapper.next_kernel_suffix(),
+                ]
             )
             # use the original src_code as the key
             wrapper.src_to_kernel[src_code] = kernel_name
@@ -3728,6 +3738,8 @@ def define_kernel(self, src_code, node_schedule, kernel):
             # to "triton_" to maximize caching opportunities (when unique_kernel_names = False).
             src_code = src_code.replace(str(Placeholder.DESCRIPTIVE_NAME), kernel_name)
             src_code = src_code.replace(str(Placeholder.KERNEL_NAME), subs_name)
+            if kernel_category == KernelCategory.TEMPLATE:
+                src_code = src_code.replace(kernel.kernel_name, kernel_name)
 
             # TODO(voz): Ostensibly, we should not need this. But there are cases where C++ codegen does
             # not use BracesBuffer, so we have no good indicator of a C++ buffer atm.
diff --git a/torch/_inductor/kernel/flex_attention.py b/torch/_inductor/kernel/flex_attention.py
index 1a64d2a89ef8b1..d084a4c80f5bc4 100644
--- a/torch/_inductor/kernel/flex_attention.py
+++ b/torch/_inductor/kernel/flex_attention.py
@@ -756,11 +756,13 @@ def _get_nv_config(query, mode: Mode) -> Tuple[int, int, int, int]:
                 return (64, 128, 8, 3)
             else:
                 return (64, 64, 4, 2)
-        elif capability >= (8, 0):  # A100
-            if head_dim == 64:
+        elif capability >= (8, 0):
+            if head_dim >= 64:
                 return (32, 128, 4, 3)
             elif head_dim == 128:
-                return (64, 128, 8, 3)
+                # SM86/89 have smaller shared memory sizes
+                num_stages = 3 if capability[-1] == 0 else 2
+                return (64, 64, 4, num_stages)
             else:
                 return (64, 64, 4, 2)
         else:  # modest hardware or extremely large head_dim
@@ -2310,9 +2312,6 @@ def flex_attention_backward(*args, **kwargs):
             or SPARSE_Q_BLOCK_SIZE % BLOCK2 != 0
         ):
             continue
-        if num_warps == 8:
-            # Working around https://github.com/pytorch/pytorch/issues/141603
-            continue
 
         # Performance tuning
         cur_kernel_options = original_kernel_options.copy()
diff --git a/torch/_inductor/metrics.py b/torch/_inductor/metrics.py
index 435fcb51a7f13b..8fbfa8dc2906c0 100644
--- a/torch/_inductor/metrics.py
+++ b/torch/_inductor/metrics.py
@@ -12,6 +12,7 @@
 
 from torch._inductor import config
 from torch._inductor.utils import get_benchmark_name
+from torch._inductor.wrapper_benchmark import KernelCategory
 from torch.utils._ordered_set import OrderedSet
 
 
@@ -292,8 +293,8 @@ def _parse_kernel_line_of_code(proper_kernel_fn_code):
     return len(proper_kernel_fn_code.splitlines())
 
 
-def _parse_size_hints(kernel_module_code, kernel_category):
-    if kernel_category == "foreach":
+def _parse_size_hints(kernel_module_code, kernel_category: KernelCategory):
+    if kernel_category == KernelCategory.FOREACH:
         # foreach kernel does not have size_hints
         return None
     m = re.search(r"size_hints=(\[[0-9, ]*\]),", kernel_module_code)
@@ -301,8 +302,11 @@ def _parse_size_hints(kernel_module_code, kernel_category):
     return m.group(1)
 
 
-def _parse_reduction_hint(kernel_category, kernel_module_code):
-    if kernel_category not in ("reduction", "persistent_reduction"):
+def _parse_reduction_hint(kernel_module_code, kernel_category: KernelCategory):
+    if kernel_category not in (
+        KernelCategory.REDUCTION,
+        KernelCategory.PERSISTENT_REDUCTION,
+    ):
         return None
     m = re.search(r"reduction_hint=ReductionHint\.(\w*),", kernel_module_code)
     assert m, "reduction_hint not found in kernel source code!"
@@ -339,7 +343,7 @@ def _parse_numel(proper_kernel_fn_code, numel_arg_name):
         return None
 
 
-def _parse_kernel_args_num_gb(kernel_fn_code, kernel_category):
+def _parse_kernel_args_num_gb(kernel_fn_code):
     """
     inductor meta looks like:
         inductor_meta={... 'mutated_arg_names': [], 'no_x_dim': False, 'kernel_num_gb': 2.0},
@@ -365,10 +369,9 @@ def log_kernel_metadata(kernel_name, kernel_path, kernel_module_code):
     It's fine to parse the generated kernel code here since the logging is
     disabled by default. It would hurt compilation time.
     """
-    from .wrapper_benchmark import get_kernel_category_by_source_code
 
-    kernel_category = get_kernel_category_by_source_code(kernel_module_code)
-    reduction_hint = _parse_reduction_hint(kernel_category, kernel_module_code)
+    kernel_category = KernelCategory.from_source_code(kernel_module_code)
+    reduction_hint = _parse_reduction_hint(kernel_module_code, kernel_category)
     size_hints = _parse_size_hints(kernel_module_code, kernel_category)
     kernel_fn_code = _parse_kernel_fn_code(kernel_module_code)
 
@@ -393,9 +396,7 @@ def log_kernel_metadata(kernel_name, kernel_path, kernel_module_code):
             "xnumel": _parse_numel(proper_kernel_fn_code, "xnumel"),
             "ynumel": _parse_numel(proper_kernel_fn_code, "ynumel"),
             "rnumel": _parse_numel(proper_kernel_fn_code, "rnumel"),
-            "kernel_args_num_gb": _parse_kernel_args_num_gb(
-                kernel_fn_code, kernel_category
-            ),
+            "kernel_args_num_gb": _parse_kernel_args_num_gb(kernel_fn_code),
         }
     )
 
diff --git a/torch/_inductor/select_algorithm.py b/torch/_inductor/select_algorithm.py
index 2fe9bf3dc2b1a1..842ad8d9944208 100644
--- a/torch/_inductor/select_algorithm.py
+++ b/torch/_inductor/select_algorithm.py
@@ -1157,7 +1157,7 @@ def generate(  # type: ignore[override]
 
         def make_kernel_render(out_node):
             kernel = TritonTemplateKernel(
-                kernel_name=str(Placeholder.KERNEL_NAME),
+                kernel_name=kernel_name,
                 output_node=out_node,
                 workspace_arg=workspace_arg,
                 use_jit=False,
diff --git a/torch/_inductor/wrapper_benchmark.py b/torch/_inductor/wrapper_benchmark.py
index e919bb09e5486b..42982d8691bc83 100644
--- a/torch/_inductor/wrapper_benchmark.py
+++ b/torch/_inductor/wrapper_benchmark.py
@@ -3,6 +3,8 @@
 import datetime
 import tempfile
 from collections import defaultdict
+from enum import Enum
+from functools import cached_property
 
 import torch
 from torch.autograd import DeviceType
@@ -12,46 +14,47 @@
 from .runtime.runtime_utils import create_bandwidth_info_str, get_num_bytes
 
 
-_kernel_category_choices = [
-    "foreach",
-    "persistent_reduction",
-    "pointwise",
-    "reduction",
-    "split_scan",
-    "template",
-]
+class KernelCategory(Enum):
+    FOREACH = "foreach"
+    PERSISTENT_REDUCTION = "persistent_reduction"
+    POINTWISE = "pointwise"
+    REDUCTION = "reduction"
+    SPLIT_SCAN = "split_scan"
+    TEMPLATE = "template"
+    UNKNOWN = "unknown"
 
+    @cached_property
+    def abbrev(self) -> str:
+        """Returns small string abbreviation for category"""
+        return self.value[:3]
 
-def get_kernel_category_by_source_code(src_code):
-    """
-    Similar to get_kernel_category but use the source code. Call this API
-    if we have not compile the src_code to module yet.
-    """
-    choices = [
-        ch for ch in _kernel_category_choices if f"@triton_heuristics.{ch}" in src_code
-    ]
-    if len(choices) == 1:
-        return choices[0]
-    else:
-        return "unknown"
+    @classmethod
+    def from_source_code(cls, src_code: str) -> "KernelCategory":
+        """
+        Similar to get_kernel_category but use the source code. Call this API
+        if we have not compile the src_code to module yet.
+        """
+        choices = [
+            cat
+            for cat in cls
+            if cat != cls.UNKNOWN and f"@triton_heuristics.{cat.value}" in src_code
+        ]
+        return choices[0] if len(choices) == 1 else cls.UNKNOWN
 
+    @classmethod
+    def from_module(cls, kernel_mod) -> "KernelCategory":
+        """
+        Given the module defining a triton kernel, return the category of the kernel.
 
-def get_kernel_category(kernel_mod):
-    """
-    Given the module defining a triton kernel, return the category of the kernel.
-    Category can be one of:
-    - pointwise
-    - reduction
-    - persistent_reduction
-
-    Currently we simply decide the category depending on what decorator is imported
-    by the kernel.
-    """
-    choices = [ch for ch in _kernel_category_choices if ch in kernel_mod.__dict__]
-    if len(choices) == 1:
-        return choices[0]
-    else:
-        return "unknown"
+        Currently we simply decide the category depending on what decorator is imported
+        by the kernel.
+        """
+        choices = [
+            cat
+            for cat in cls
+            if cat != cls.UNKNOWN and cat.value in kernel_mod.__dict__
+        ]
+        return choices[0] if len(choices) == 1 else cls.UNKNOWN
 
 
 def get_triton_kernel(mod):
@@ -85,7 +88,7 @@ def benchmark_all_kernels(benchmark_name, benchmark_all_configs):
             continue
 
         triton_kernel = get_triton_kernel(kernel_mod)
-        kernel_category = get_kernel_category(kernel_mod)
+        kernel_category = KernelCategory.from_module(kernel_mod)
         args = kernel_mod.get_args()
         num_in_out_ptrs = len(
             [
@@ -112,7 +115,7 @@ def get_info_str(ms, n_regs, n_spills, shared, prefix=""):
             )
 
         kernel_desc = (
-            f"{benchmark_name:20} {kernel_category[:3].upper()} {kernel_key[:10]}"
+            f"{benchmark_name:20} {kernel_category.abbrev.upper()} {kernel_key[:10]}"
         )
         if benchmark_all_configs:
             assert hasattr(kernel_mod, "benchmark_all_configs")
@@ -396,7 +399,7 @@ def compiled_module_main(benchmark_name, benchmark_compiled_module_fn):
 
         if torch.cuda.is_available():
             peak_mem = torch.cuda.max_memory_allocated()
-            print(f"Peak GPU memory usage {peak_mem/1e6:.3f} MB")
+            print(f"Peak GPU memory usage {peak_mem / 1e6:.3f} MB")
 
         if torch.cuda.is_available() and args.cuda_memory_snapshot:
             collect_memory_snapshot(benchmark_compiled_module_fn)
